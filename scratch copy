import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
import librosa

class MultimodalAttention(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads, dropout=0.1):
        super(MultimodalAttention, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)
        self.linear = nn.Linear(hidden_dim, hidden_dim)
        self.text_encoder = nn.TransformerEncoderLayer(hidden_dim, num_heads, dropout=dropout)
        self.multihead_attn_c3 = nn.MultiheadAttention(hidden_dim, 10, dropout=dropout)

    def forward(self, Xt, Xa, Xv):
        ft = self.embedding(Xt)
        fa = self.embedding(Xa)
        #fv = self.embedding(Xv)
        Qa = self.query(fa)
        Ka = self.key(fa)
        Va = self.value(fa)
        #Qv = self.query(fv)
        #Kv = self.key(fv)
        #Vv = self.value(fv)
        Atten_a, _ = self.multihead_attn(Qa, Ka, Va)
        #Atten_v, _ = self.multihead_attn(Qv, Kv, Vv)

        #F1 = ft + Atten_a + Atten_v
        F1 = ft + Atten_a

        F1 = self.linear(F1)
        
        #F2 = torch.cat([Atten_a, Atten_v], dim=1)
        F2 = Atten_a

        ft_text_encoded = self.text_encoder(ft)
        F3 = self.multihead_attn_c3(F1, ft_text_encoded, ft_text_encoded)
        return F1, F2, F3
    

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')



data_path = "/Users/dinesh/College/final proj/attempt3/updatedMoseiData"
df = pd.read_csv(f"{data_path}/processed_mosei.csv")

# Extract text, audio paths, and labels
text_data = df["text"].tolist()
audio_paths = [f"{data_path}/audio/{file}.wav" for file in df["file_name"]]
labels = df["sentiment_label"].tolist()

# Encode labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

class AudioTextDataset(Dataset):
    def __init__(self, text_data, audio_paths, labels):
        self.text_data = text_data
        self.audio_paths = audio_paths
        self.labels = labels

    def __len__(self):
        return len(self.text_data)

    def __getitem__(self, idx):
        text = self.text_data[idx]
        audio_path = self.audio_paths[idx]
        label = self.labels[idx]

        # Load audio and extract features
        y, sr = librosa.load(audio_path)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        rms = librosa.feature.rms(y=y)
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)
        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)

        # Combine audio features
        audio_features = np.concatenate((mfcc.mean(axis=1), rms.mean(axis=1),
                                         spectral_centroid.mean(axis=1),
                                         zero_crossing_rate.mean(axis=1),
                                         rolloff.mean(axis=1)))

        # Tokenize and encode text using a pre-trained language model
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt')

        return input_ids, audio_features, label
    
dataset = AudioTextDataset(text_data, audio_paths, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

